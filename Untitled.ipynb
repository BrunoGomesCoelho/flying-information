{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import used libraries\n",
    "import sys # For some basic error checks since we are not implementing exceptions\n",
    "\n",
    "# For scraping/crawling\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import certifi\n",
    "import re\n",
    "\n",
    "# For converting strings to dictionaries\n",
    "import json\n",
    "\n",
    "\n",
    "ERROR_MESSAGES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a better name for sys function\n",
    "program_current_line = sys._getframe().f_lineno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A static count for now; better would be to actually scrape and click on the next page until no longe possible\n",
    "BASE_REFRIGERATOR_PAGE = \"https://www.magazineluiza.com.br/geladeira-refrigerador/eletrodomesticos/s/ed/refr/\"\n",
    "REFRIGERATOR_PAGES_COUNT = 6\n",
    "\n",
    "BASE_WASHER_PAGE = \"https://www.magazineluiza.com.br/lavadora-de-roupas-lava-e-seca/eletrodomesticos/s/ed/ela1/\"\n",
    "WASHER_PAGES_COUNT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(url):\n",
    "    http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n",
    "    headers = {}\n",
    "    headers[\"User-Agent\"] = \"Mozilla/5.0 (X11; Linux x86_64) \\\n",
    "            \\AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "    headers[\"X-Requested-With\"] = \"XMLHttpRequest\"\n",
    "\n",
    "    # Try to get the page and the soup.\n",
    "    # A better implementation would use try/except to catch errors.\n",
    "    req = http.request(\n",
    "        \"GET\",\n",
    "        url,\n",
    "        headers=headers\n",
    "    )\n",
    "\n",
    "    page = req.data\n",
    "    return BeautifulSoup(page, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Appliance(object):\n",
    "    \"\"\"\n",
    "    The Appliance class holds all usefull information for a appliance.\n",
    "    Some of this information is pertinent to the customer, some of it is pertinent for our IA model.\n",
    "    \"\"\"\n",
    "    def __init__(self, code, title, brand, model, price, category):\n",
    "        self.code = code\n",
    "        self.title = title\n",
    "        self.brand = brand\n",
    "        self.model = model\n",
    "        self.price = price\n",
    "        self.category = category\n",
    "        \n",
    "    def get_customer_info(self):\n",
    "        return str(self.code), self.title, self.brand, self.model, self.price, self.category\n",
    "\n",
    "    def __str__(self):\n",
    "        return \", \".join(self.get_customer_info())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was here\n",
    "\n",
    "def get_product_info(product):\n",
    "    # Json converts our string mess into a easy to use dictionary\n",
    "    info_dict = json.loads(product.find(\"a\")[\"data-product\"])\n",
    "    product_link = product.find(\"a\")[\"href\"]\n",
    "\n",
    "    try:\n",
    "        # Follow the link for the product model since it frequently can't be found on the search\n",
    "        # A better idea would be to use string searches/regex probably, done this way due to time constraints.\n",
    "        product_soup = connect(product_link)\n",
    "        product_model = product_soup.find(\"table\", {\"class\": \"description__box--wildSand\"})\\\n",
    "                        .find(\"tr\").find_all(\"td\")[1].find_all(\"table\")[2]\\\n",
    "                        .find_all(\"td\")[1].text\n",
    "        product_model = product_model.strip()  # Remove white spaces\n",
    "        info_dict[\"model\"] = product_model\n",
    "    except:\n",
    "        ERROR_MESSAGES.append(\"No model information found for {}\".format(refrigerator_link))\n",
    "    \n",
    "    # If we dont find the model, use the reference since it may contain the model\n",
    "    return Appliance(code=int(info_dict[\"product\"]), title=info_dict[\"title\"],\n",
    "                     brand=info_dict[\"brand\"], model=info_dict[\"model\"], \n",
    "                     price=info_dict[\"price\"], category=info_dict[\"category\"])\n",
    "\n",
    "def scrape_refrigerators():\n",
    "    refrigerators = []\n",
    "    #washers\n",
    "    \n",
    "    for page_index in range(1, REFRIGERATOR_PAGES_COUNT+1):\n",
    "            soup = connect(BASE_REFRIGERATOR_PAGE + \"{}/\".format(page_index))\n",
    "            content = soup.find_all(\"ul\", {\"class\": \"productShowCase big\"})\n",
    "\n",
    "            if len(content) != 1:\n",
    "                ERROR_MESSAGES.append(\"Line {}: productShowCase big has changed\".format(program_current_line))\n",
    "            \n",
    "            product_list = content[0].find_all(\"li\")\n",
    "\n",
    "            # Remove the last item as it does not represent a product\n",
    "            product_list = product_list[:-1]\n",
    "       \n",
    "            for product in product_list[:10]:\n",
    "                # Json converts our string mess into a easy to use dictionary\n",
    "                refrigerators.append(get_product_info(product))\n",
    "    return refrigerators, washers\n",
    "                            \n",
    "\n",
    "all_refrigerators = scrape_refrigerators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 refrigerators scraped!\n"
     ]
    }
   ],
   "source": [
    "print(\"{} refrigerators scraped!\".format(len(all_refrigerators)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105418, Lava e Seca LG 11kg Smart Care Prime Touch , lg, Prime Touch, 2799.9, ed 2190089, Lava e Seca Samsung 10,2kg Prata WD4000, samsung, WD4000, 2699.9, ed 2159005, Lava e Seca Samsung 10,2kg Inox WD6000, samsung, WD6000, 3299.9, ed 2190087, Lava e Seca Samsung 10,2kg WD4000, samsung, WD4000, 2449.9, ed 2190566, Lava e Seca Midea 12Kg Prata Acqua, midea, Acqua, 2499.9, ed 105416, Lava e Seca LG 11kg Smart Care Prime Touch , lg, Prime Touch, 3199.9, ed 2173042, Lava e Seca LG 8,5kg Mega Touch, lg, Mega Touch, 2299.9, ed 105414, Lava e Seca Samsung 10,2Kg WD10J, samsung, WD10J, 2949.9, ed 2190085, Lava e Seca Samsung 8,5kg WD4000, samsung, WD4000, 2399.9, ed 2029729, Lava e Seca Samsung 10,1kg WD106UHSAWQ, samsung, WD106UHSAWQ, None, ed\n"
     ]
    }
   ],
   "source": [
    "print(*all_refrigerators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105418\n",
      "105418, Lava e Seca LG 11kg Smart Care Prime Touch , lg, Prime Touch, 2799.9, ed\n",
      "2190089, Lava e Seca Samsung 10,2kg Prata WD4000, samsung, WD4000, 2699.9, ed\n",
      "2159005, Lava e Seca Samsung 10,2kg Inox WD6000, samsung, WD6000, 3299.9, ed\n",
      "2190087, Lava e Seca Samsung 10,2kg WD4000, samsung, WD4000, 2449.9, ed\n",
      "2190566, Lava e Seca Midea 12Kg Prata Acqua, midea, Acqua, 2499.9, ed\n",
      "105416, Lava e Seca LG 11kg Smart Care Prime Touch , lg, Prime Touch, 3199.9, ed\n",
      "2173042, Lava e Seca LG 8,5kg Mega Touch, lg, Mega Touch, 2299.9, ed\n",
      "105414, Lava e Seca Samsung 10,2Kg WD10J, samsung, WD10J, 2949.9, ed\n",
      "2190085, Lava e Seca Samsung 8,5kg WD4000, samsung, WD4000, 2399.9, ed\n",
      "2029729, Lava e Seca Samsung 10,1kg WD106UHSAWQ, samsung, WD106UHSAWQ, None, ed\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "print(str(all_refrigerators[0].code))\n",
    "\n",
    "with open(\"refrigerators.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    content_writer = csv.writer(csvfile, delimiter=' ',\n",
    "                        quotechar='\"')\n",
    "    for item in all_refrigerators:\n",
    "        print(item)\n",
    "        content_writer.writerow([str(x) for x in item.get_customer_info()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
